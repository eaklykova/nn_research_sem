{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа №3\n",
    "## Выполнила Елизавета Клыкова, БКЛ181\n",
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:01:22.814621Z",
     "iopub.status.busy": "2021-12-20T19:01:22.814056Z",
     "iopub.status.idle": "2021-12-20T19:01:22.836208Z",
     "shell.execute_reply": "2021-12-20T19:01:22.835507Z",
     "shell.execute_reply.started": "2021-12-20T19:01:22.814525Z"
    },
    "id": "e5BgHdtW2sO3"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:01:25.789557Z",
     "iopub.status.busy": "2021-12-20T19:01:25.789044Z",
     "iopub.status.idle": "2021-12-20T19:01:33.438011Z",
     "shell.execute_reply": "2021-12-20T19:01:33.437283Z",
     "shell.execute_reply.started": "2021-12-20T19:01:25.789520Z"
    },
    "id": "qmzaEwy9W-Ae"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset,\\\n",
    "    RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:01:38.511706Z",
     "iopub.status.busy": "2021-12-20T19:01:38.511236Z",
     "iopub.status.idle": "2021-12-20T19:01:38.520118Z",
     "shell.execute_reply": "2021-12-20T19:01:38.519307Z",
     "shell.execute_reply.started": "2021-12-20T19:01:38.511668Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 117\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "Я перенесла нужные ячейки в эту тетрадку вместо того, чтобы запускать исходную (мне кажется, так легче воспринимать код и внесенные изменения).\n",
    "\n",
    "#### Загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:01:57.895623Z",
     "iopub.status.busy": "2021-12-20T19:01:57.895361Z",
     "iopub.status.idle": "2021-12-20T19:01:59.241712Z",
     "shell.execute_reply": "2021-12-20T19:01:59.240971Z",
     "shell.execute_reply.started": "2021-12-20T19:01:57.895596Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/answers-subsample/answers_subsample.csv')\n",
    "cat_mapper = {cat: n for n, cat in enumerate(data.category.unique())}\n",
    "data.category = data.category.map(cat_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:02:02.339559Z",
     "iopub.status.busy": "2021-12-20T19:02:02.338939Z",
     "iopub.status.idle": "2021-12-20T19:02:07.179411Z",
     "shell.execute_reply": "2021-12-20T19:02:07.178667Z",
     "shell.execute_reply.started": "2021-12-20T19:02:02.339522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea377da474ab412598d9ed9f205f0027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9d7f2efb70473f895b3e6a02133b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb469935cb34753b97caa4b3d085d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.64M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6ea881971941b6aafc21bd198e1852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased',\n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:02:14.528124Z",
     "iopub.status.busy": "2021-12-20T19:02:14.527845Z",
     "iopub.status.idle": "2021-12-20T19:02:14.532197Z",
     "shell.execute_reply": "2021-12-20T19:02:14.531540Z",
     "shell.execute_reply.started": "2021-12-20T19:02:14.528097Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = data.text.values\n",
    "labels = data.category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:02:17.542579Z",
     "iopub.status.busy": "2021-12-20T19:02:17.542014Z",
     "iopub.status.idle": "2021-12-20T19:02:17.554808Z",
     "shell.execute_reply": "2021-12-20T19:02:17.553979Z",
     "shell.execute_reply.started": "2021-12-20T19:02:17.542542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Могут ли в россельхозбанке дать в залог норковых шуб помогите пожалуйста\n",
      "\n",
      "Tokenized:  ['могут', 'ли', 'в', 'рос', '##сель', '##хо', '##з', '##бан', '##ке', 'да', '##ть', 'в', 'зал', '##ог', 'но', '##рк', '##овых', 'ш', '##уб', 'пом', '##оги', '##те', 'по', '##жал', '##уи', '##ста']\n",
      "\n",
      "Token IDs:  [22553, 23029, 309, 26673, 80686, 37489, 11637, 42572, 11827, 10448, 11569, 309, 53932, 19820, 11299, 53464, 20565, 330, 58675, 86074, 60338, 10740, 10291, 28704, 62848, 15294]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print('Original: ', sentences[0])\n",
    "print()\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "print()\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(\n",
    "    tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизация + индексация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:02:22.517350Z",
     "iopub.status.busy": "2021-12-20T19:02:22.516797Z",
     "iopub.status.idle": "2021-12-20T19:04:43.434988Z",
     "shell.execute_reply": "2021-12-20T19:04:43.434283Z",
     "shell.execute_reply.started": "2021-12-20T19:02:22.517316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a868a5eef60444c6a0d2491bf0f2ad45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237779 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Могут ли в россельхозбанке дать в залог норковых шуб помогите пожалуйста\n",
      "Token IDs: [101, 22553, 23029, 309, 26673, 80686, 37489, 11637, 42572, 11827, 10448, 11569, 309, 53932, 19820, 11299, 53464, 20565, 330, 58675, 86074, 60338, 10740, 10291, 28704, 62848, 15294, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        # max_length = 128,          # Truncate all sentences.\n",
    "                        # return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Паддинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:04:47.769790Z",
     "iopub.status.busy": "2021-12-20T19:04:47.769533Z",
     "iopub.status.idle": "2021-12-20T19:04:47.804071Z",
     "shell.execute_reply": "2021-12-20T19:04:47.803149Z",
     "shell.execute_reply.started": "2021-12-20T19:04:47.769763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 97\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length:', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:04:52.835649Z",
     "iopub.status.busy": "2021-12-20T19:04:52.835355Z",
     "iopub.status.idle": "2021-12-20T19:04:55.140753Z",
     "shell.execute_reply": "2021-12-20T19:04:55.140021Z",
     "shell.execute_reply.started": "2021-12-20T19:04:52.835621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 97 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum sequence length to 97.\n",
    "MAX_LEN = 97\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token,\n",
    "                                               tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-маски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:05:05.298639Z",
     "iopub.status.busy": "2021-12-20T19:05:05.298370Z",
     "iopub.status.idle": "2021-12-20T19:05:29.471699Z",
     "shell.execute_reply": "2021-12-20T19:05:29.470885Z",
     "shell.execute_reply.started": "2021-12-20T19:05:05.298610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83124bd8b384c76bf14509a56c9ddcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237779 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in tqdm(input_ids):\n",
    "\n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Делим датасет\n",
    "Тут будет финт ушами: нужно урезать обучающую выборку, чтобы не ждать выполнения кода до завтра, но не урезать валидационную выборку. Сократим трейновые данные в два раза (с 90% до 45% от общего объема)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:05:32.346581Z",
     "iopub.status.busy": "2021-12-20T19:05:32.346104Z",
     "iopub.status.idle": "2021-12-20T19:05:32.459466Z",
     "shell.execute_reply": "2021-12-20T19:05:32.458765Z",
     "shell.execute_reply.started": "2021-12-20T19:05:32.346546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use 45% for training and 10% for validation.\n",
    "train_inputs, val_inputs, train_labels, val_labels = \\\n",
    "    train_test_split(input_ids, labels, random_state=seed,\n",
    "                     test_size=0.1, train_size=0.45)\n",
    "\n",
    "# Do the same for the masks.\n",
    "train_masks, val_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                                random_state=seed,\n",
    "                                                test_size=0.1,\n",
    "                                                train_size=0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Конвертация в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:05:45.495726Z",
     "iopub.status.busy": "2021-12-20T19:05:45.495423Z",
     "iopub.status.idle": "2021-12-20T19:05:47.097192Z",
     "shell.execute_reply": "2021-12-20T19:05:47.096388Z",
     "shell.execute_reply.started": "2021-12-20T19:05:45.495695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors,\n",
    "# the required datatype for our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:05:50.217530Z",
     "iopub.status.busy": "2021-12-20T19:05:50.216946Z",
     "iopub.status.idle": "2021-12-20T19:05:50.224150Z",
     "shell.execute_reply": "2021-12-20T19:05:50.223146Z",
     "shell.execute_reply.started": "2021-12-20T19:05:50.217486Z"
    }
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training,\n",
    "# so we specify it here.\n",
    "# For fine-tuning BERT on a specific task,\n",
    "# the authors recommend a batch size of 16 or 32.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler,\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:06:16.287274Z",
     "iopub.status.busy": "2021-12-20T19:06:16.286743Z",
     "iopub.status.idle": "2021-12-20T19:06:44.346536Z",
     "shell.execute_reply": "2021-12-20T19:06:44.345864Z",
     "shell.execute_reply.started": "2021-12-20T19:06:16.287240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7494f8cdce74cd0814dadbfeecbab94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/641M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
    "# linear classification layer on top.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-uncased\",  # 12-layer model with an uncased vocab\n",
    "    num_labels=len(data.category.unique()),  # The number of output labels\n",
    "    output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:06:51.395752Z",
     "iopub.status.busy": "2021-12-20T19:06:51.395494Z",
     "iopub.status.idle": "2021-12-20T19:06:51.402277Z",
     "shell.execute_reply": "2021-12-20T19:06:51.401442Z",
     "shell.execute_reply.started": "2021-12-20T19:06:51.395725Z"
    }
   },
   "outputs": [],
   "source": [
    "b = model.bert.pooler.dense.weight\n",
    "c = model.classifier.weight\n",
    "b = b.cpu().detach().numpy()\n",
    "c = c.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:06:54.540577Z",
     "iopub.status.busy": "2021-12-20T19:06:54.540312Z",
     "iopub.status.idle": "2021-12-20T19:06:54.556188Z",
     "shell.execute_reply": "2021-12-20T19:06:54.555481Z",
     "shell.execute_reply.started": "2021-12-20T19:06:54.540548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (105879, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (5, 768)\n",
      "classifier.bias                                                 (5,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print(\n",
    "    'The BERT model has {:} different named parameters.\\n'.format(\n",
    "        len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:07:00.245055Z",
     "iopub.status.busy": "2021-12-20T19:07:00.244269Z",
     "iopub.status.idle": "2021-12-20T19:07:00.252966Z",
     "shell.execute_reply": "2021-12-20T19:07:00.252240Z",
     "shell.execute_reply.started": "2021-12-20T19:07:00.245010Z"
    }
   },
   "outputs": [],
   "source": [
    "# AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,  # args.learning_rate - default is 5e-5\n",
    "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:07:08.977245Z",
     "iopub.status.busy": "2021-12-20T19:07:08.976701Z",
     "iopub.status.idle": "2021-12-20T19:07:08.981531Z",
     "shell.execute_reply": "2021-12-20T19:07:08.980852Z",
     "shell.execute_reply.started": "2021-12-20T19:07:08.977206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=100,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "#### Подсчет ошибки\n",
    "В оригинальной тетрадке accuracy, мы возьмем f1, чтобы метрики были сравнимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:07:15.837554Z",
     "iopub.status.busy": "2021-12-20T19:07:15.837291Z",
     "iopub.status.idle": "2021-12-20T19:07:15.844093Z",
     "shell.execute_reply": "2021-12-20T19:07:15.843357Z",
     "shell.execute_reply.started": "2021-12-20T19:07:15.837524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the f1 of our predictions vs labels\n",
    "def flat_f1_score(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(pred_flat, labels_flat, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:07:22.446385Z",
     "iopub.status.busy": "2021-12-20T19:07:22.446122Z",
     "iopub.status.idle": "2021-12-20T19:07:22.451297Z",
     "shell.execute_reply": "2021-12-20T19:07:22.450577Z",
     "shell.execute_reply.started": "2021-12-20T19:07:22.446357Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    \"\"\"\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    \"\"\"\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:07:26.309404Z",
     "iopub.status.busy": "2021-12-20T19:07:26.308851Z",
     "iopub.status.idle": "2021-12-20T19:07:26.314566Z",
     "shell.execute_reply": "2021-12-20T19:07:26.313904Z",
     "shell.execute_reply.started": "2021-12-20T19:07:26.309366Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available()\\\n",
    "    else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T19:26:38.468343Z",
     "iopub.status.busy": "2021-12-20T19:26:38.467944Z",
     "iopub.status.idle": "2021-12-20T20:23:39.392293Z",
     "shell.execute_reply": "2021-12-20T20:23:39.391519Z",
     "shell.execute_reply.started": "2021-12-20T19:26:38.468309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  3,344.    Elapsed: 0:00:13.\n",
      "  Batch    80  of  3,344.    Elapsed: 0:00:26.\n",
      "  Batch   120  of  3,344.    Elapsed: 0:00:38.\n",
      "  Batch   160  of  3,344.    Elapsed: 0:00:51.\n",
      "  Batch   200  of  3,344.    Elapsed: 0:01:04.\n",
      "  Batch   240  of  3,344.    Elapsed: 0:01:17.\n",
      "  Batch   280  of  3,344.    Elapsed: 0:01:29.\n",
      "  Batch   320  of  3,344.    Elapsed: 0:01:42.\n",
      "  Batch   360  of  3,344.    Elapsed: 0:01:55.\n",
      "  Batch   400  of  3,344.    Elapsed: 0:02:08.\n",
      "  Batch   440  of  3,344.    Elapsed: 0:02:21.\n",
      "  Batch   480  of  3,344.    Elapsed: 0:02:33.\n",
      "  Batch   520  of  3,344.    Elapsed: 0:02:46.\n",
      "  Batch   560  of  3,344.    Elapsed: 0:02:59.\n",
      "  Batch   600  of  3,344.    Elapsed: 0:03:12.\n",
      "  Batch   640  of  3,344.    Elapsed: 0:03:24.\n",
      "  Batch   680  of  3,344.    Elapsed: 0:03:37.\n",
      "  Batch   720  of  3,344.    Elapsed: 0:03:50.\n",
      "  Batch   760  of  3,344.    Elapsed: 0:04:03.\n",
      "  Batch   800  of  3,344.    Elapsed: 0:04:15.\n",
      "  Batch   840  of  3,344.    Elapsed: 0:04:28.\n",
      "  Batch   880  of  3,344.    Elapsed: 0:04:41.\n",
      "  Batch   920  of  3,344.    Elapsed: 0:04:54.\n",
      "  Batch   960  of  3,344.    Elapsed: 0:05:06.\n",
      "  Batch 1,000  of  3,344.    Elapsed: 0:05:19.\n",
      "  Batch 1,040  of  3,344.    Elapsed: 0:05:32.\n",
      "  Batch 1,080  of  3,344.    Elapsed: 0:05:45.\n",
      "  Batch 1,120  of  3,344.    Elapsed: 0:05:57.\n",
      "  Batch 1,160  of  3,344.    Elapsed: 0:06:10.\n",
      "  Batch 1,200  of  3,344.    Elapsed: 0:06:23.\n",
      "  Batch 1,240  of  3,344.    Elapsed: 0:06:36.\n",
      "  Batch 1,280  of  3,344.    Elapsed: 0:06:48.\n",
      "  Batch 1,320  of  3,344.    Elapsed: 0:07:01.\n",
      "  Batch 1,360  of  3,344.    Elapsed: 0:07:14.\n",
      "  Batch 1,400  of  3,344.    Elapsed: 0:07:27.\n",
      "  Batch 1,440  of  3,344.    Elapsed: 0:07:39.\n",
      "  Batch 1,480  of  3,344.    Elapsed: 0:07:52.\n",
      "  Batch 1,520  of  3,344.    Elapsed: 0:08:05.\n",
      "  Batch 1,560  of  3,344.    Elapsed: 0:08:18.\n",
      "  Batch 1,600  of  3,344.    Elapsed: 0:08:30.\n",
      "  Batch 1,640  of  3,344.    Elapsed: 0:08:43.\n",
      "  Batch 1,680  of  3,344.    Elapsed: 0:08:56.\n",
      "  Batch 1,720  of  3,344.    Elapsed: 0:09:09.\n",
      "  Batch 1,760  of  3,344.    Elapsed: 0:09:21.\n",
      "  Batch 1,800  of  3,344.    Elapsed: 0:09:34.\n",
      "  Batch 1,840  of  3,344.    Elapsed: 0:09:47.\n",
      "  Batch 1,880  of  3,344.    Elapsed: 0:10:00.\n",
      "  Batch 1,920  of  3,344.    Elapsed: 0:10:12.\n",
      "  Batch 1,960  of  3,344.    Elapsed: 0:10:25.\n",
      "  Batch 2,000  of  3,344.    Elapsed: 0:10:38.\n",
      "  Batch 2,040  of  3,344.    Elapsed: 0:10:51.\n",
      "  Batch 2,080  of  3,344.    Elapsed: 0:11:03.\n",
      "  Batch 2,120  of  3,344.    Elapsed: 0:11:16.\n",
      "  Batch 2,160  of  3,344.    Elapsed: 0:11:29.\n",
      "  Batch 2,200  of  3,344.    Elapsed: 0:11:42.\n",
      "  Batch 2,240  of  3,344.    Elapsed: 0:11:54.\n",
      "  Batch 2,280  of  3,344.    Elapsed: 0:12:07.\n",
      "  Batch 2,320  of  3,344.    Elapsed: 0:12:20.\n",
      "  Batch 2,360  of  3,344.    Elapsed: 0:12:33.\n",
      "  Batch 2,400  of  3,344.    Elapsed: 0:12:46.\n",
      "  Batch 2,440  of  3,344.    Elapsed: 0:12:58.\n",
      "  Batch 2,480  of  3,344.    Elapsed: 0:13:11.\n",
      "  Batch 2,520  of  3,344.    Elapsed: 0:13:24.\n",
      "  Batch 2,560  of  3,344.    Elapsed: 0:13:37.\n",
      "  Batch 2,600  of  3,344.    Elapsed: 0:13:50.\n",
      "  Batch 2,640  of  3,344.    Elapsed: 0:14:02.\n",
      "  Batch 2,680  of  3,344.    Elapsed: 0:14:15.\n",
      "  Batch 2,720  of  3,344.    Elapsed: 0:14:28.\n",
      "  Batch 2,760  of  3,344.    Elapsed: 0:14:41.\n",
      "  Batch 2,800  of  3,344.    Elapsed: 0:14:53.\n",
      "  Batch 2,840  of  3,344.    Elapsed: 0:15:06.\n",
      "  Batch 2,880  of  3,344.    Elapsed: 0:15:19.\n",
      "  Batch 2,920  of  3,344.    Elapsed: 0:15:32.\n",
      "  Batch 2,960  of  3,344.    Elapsed: 0:15:45.\n",
      "  Batch 3,000  of  3,344.    Elapsed: 0:15:57.\n",
      "  Batch 3,040  of  3,344.    Elapsed: 0:16:10.\n",
      "  Batch 3,080  of  3,344.    Elapsed: 0:16:23.\n",
      "  Batch 3,120  of  3,344.    Elapsed: 0:16:36.\n",
      "  Batch 3,160  of  3,344.    Elapsed: 0:16:48.\n",
      "  Batch 3,200  of  3,344.    Elapsed: 0:17:01.\n",
      "  Batch 3,240  of  3,344.    Elapsed: 0:17:14.\n",
      "  Batch 3,280  of  3,344.    Elapsed: 0:17:27.\n",
      "  Batch 3,320  of  3,344.    Elapsed: 0:17:40.\n",
      "\n",
      "  Average training loss: 0.40\n",
      "  Training epcoh took: 0:17:47\n",
      "\n",
      "Running Validation...\n",
      "  F1 score: 0.84\n",
      "  Validation took: 0:01:13\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  3,344.    Elapsed: 0:00:13.\n",
      "  Batch    80  of  3,344.    Elapsed: 0:00:26.\n",
      "  Batch   120  of  3,344.    Elapsed: 0:00:38.\n",
      "  Batch   160  of  3,344.    Elapsed: 0:00:51.\n",
      "  Batch   200  of  3,344.    Elapsed: 0:01:04.\n",
      "  Batch   240  of  3,344.    Elapsed: 0:01:17.\n",
      "  Batch   280  of  3,344.    Elapsed: 0:01:29.\n",
      "  Batch   320  of  3,344.    Elapsed: 0:01:42.\n",
      "  Batch   360  of  3,344.    Elapsed: 0:01:55.\n",
      "  Batch   400  of  3,344.    Elapsed: 0:02:08.\n",
      "  Batch   440  of  3,344.    Elapsed: 0:02:20.\n",
      "  Batch   480  of  3,344.    Elapsed: 0:02:33.\n",
      "  Batch   520  of  3,344.    Elapsed: 0:02:46.\n",
      "  Batch   560  of  3,344.    Elapsed: 0:02:59.\n",
      "  Batch   600  of  3,344.    Elapsed: 0:03:11.\n",
      "  Batch   640  of  3,344.    Elapsed: 0:03:24.\n",
      "  Batch   680  of  3,344.    Elapsed: 0:03:37.\n",
      "  Batch   720  of  3,344.    Elapsed: 0:03:50.\n",
      "  Batch   760  of  3,344.    Elapsed: 0:04:03.\n",
      "  Batch   800  of  3,344.    Elapsed: 0:04:15.\n",
      "  Batch   840  of  3,344.    Elapsed: 0:04:28.\n",
      "  Batch   920  of  3,344.    Elapsed: 0:04:54.\n",
      "  Batch   960  of  3,344.    Elapsed: 0:05:07.\n",
      "  Batch 1,000  of  3,344.    Elapsed: 0:05:19.\n",
      "  Batch 1,040  of  3,344.    Elapsed: 0:05:32.\n",
      "  Batch 1,080  of  3,344.    Elapsed: 0:05:45.\n",
      "  Batch 1,120  of  3,344.    Elapsed: 0:05:58.\n",
      "  Batch 1,160  of  3,344.    Elapsed: 0:06:11.\n",
      "  Batch 1,200  of  3,344.    Elapsed: 0:06:23.\n",
      "  Batch 1,240  of  3,344.    Elapsed: 0:06:36.\n",
      "  Batch 1,280  of  3,344.    Elapsed: 0:06:49.\n",
      "  Batch 1,320  of  3,344.    Elapsed: 0:07:02.\n",
      "  Batch 1,360  of  3,344.    Elapsed: 0:07:14.\n",
      "  Batch 1,400  of  3,344.    Elapsed: 0:07:27.\n",
      "  Batch 1,440  of  3,344.    Elapsed: 0:07:40.\n",
      "  Batch 1,480  of  3,344.    Elapsed: 0:07:53.\n",
      "  Batch 1,520  of  3,344.    Elapsed: 0:08:05.\n",
      "  Batch 1,560  of  3,344.    Elapsed: 0:08:18.\n",
      "  Batch 1,600  of  3,344.    Elapsed: 0:08:31.\n",
      "  Batch 1,640  of  3,344.    Elapsed: 0:08:44.\n",
      "  Batch 1,680  of  3,344.    Elapsed: 0:08:57.\n",
      "  Batch 1,720  of  3,344.    Elapsed: 0:09:09.\n",
      "  Batch 1,760  of  3,344.    Elapsed: 0:09:22.\n",
      "  Batch 1,800  of  3,344.    Elapsed: 0:09:35.\n",
      "  Batch 1,840  of  3,344.    Elapsed: 0:09:48.\n",
      "  Batch 1,880  of  3,344.    Elapsed: 0:10:00.\n",
      "  Batch 1,920  of  3,344.    Elapsed: 0:10:13.\n",
      "  Batch 1,960  of  3,344.    Elapsed: 0:10:26.\n",
      "  Batch 2,000  of  3,344.    Elapsed: 0:10:39.\n",
      "  Batch 2,040  of  3,344.    Elapsed: 0:10:51.\n",
      "  Batch 2,080  of  3,344.    Elapsed: 0:11:04.\n",
      "  Batch 2,120  of  3,344.    Elapsed: 0:11:17.\n",
      "  Batch 2,160  of  3,344.    Elapsed: 0:11:30.\n",
      "  Batch 2,200  of  3,344.    Elapsed: 0:11:42.\n",
      "  Batch 2,240  of  3,344.    Elapsed: 0:11:55.\n",
      "  Batch 2,280  of  3,344.    Elapsed: 0:12:08.\n",
      "  Batch 2,320  of  3,344.    Elapsed: 0:12:21.\n",
      "  Batch 2,360  of  3,344.    Elapsed: 0:12:33.\n",
      "  Batch 2,400  of  3,344.    Elapsed: 0:12:46.\n",
      "  Batch 2,440  of  3,344.    Elapsed: 0:12:59.\n",
      "  Batch 2,480  of  3,344.    Elapsed: 0:13:12.\n",
      "  Batch 2,520  of  3,344.    Elapsed: 0:13:25.\n",
      "  Batch 2,560  of  3,344.    Elapsed: 0:13:37.\n",
      "  Batch 2,600  of  3,344.    Elapsed: 0:13:50.\n",
      "  Batch 2,640  of  3,344.    Elapsed: 0:14:03.\n",
      "  Batch 2,680  of  3,344.    Elapsed: 0:14:16.\n",
      "  Batch 2,720  of  3,344.    Elapsed: 0:14:28.\n",
      "  Batch 2,760  of  3,344.    Elapsed: 0:14:41.\n",
      "  Batch 2,800  of  3,344.    Elapsed: 0:14:54.\n",
      "  Batch 2,840  of  3,344.    Elapsed: 0:15:07.\n",
      "  Batch 2,880  of  3,344.    Elapsed: 0:15:19.\n",
      "  Batch 2,920  of  3,344.    Elapsed: 0:15:32.\n",
      "  Batch 2,960  of  3,344.    Elapsed: 0:15:45.\n",
      "  Batch 3,000  of  3,344.    Elapsed: 0:15:58.\n",
      "  Batch 3,040  of  3,344.    Elapsed: 0:16:11.\n",
      "  Batch 3,080  of  3,344.    Elapsed: 0:16:23.\n",
      "  Batch 3,120  of  3,344.    Elapsed: 0:16:36.\n",
      "  Batch 3,160  of  3,344.    Elapsed: 0:16:49.\n",
      "  Batch 3,200  of  3,344.    Elapsed: 0:17:02.\n",
      "  Batch 3,240  of  3,344.    Elapsed: 0:17:14.\n",
      "  Batch 3,280  of  3,344.    Elapsed: 0:17:27.\n",
      "  Batch 3,320  of  3,344.    Elapsed: 0:17:40.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epcoh took: 0:17:47\n",
      "\n",
      "Running Validation...\n",
      "  F1 score: 0.84\n",
      "  Validation took: 0:01:13\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  3,344.    Elapsed: 0:00:13.\n",
      "  Batch    80  of  3,344.    Elapsed: 0:00:26.\n",
      "  Batch   120  of  3,344.    Elapsed: 0:00:38.\n",
      "  Batch   160  of  3,344.    Elapsed: 0:00:51.\n",
      "  Batch   200  of  3,344.    Elapsed: 0:01:04.\n",
      "  Batch   240  of  3,344.    Elapsed: 0:01:17.\n",
      "  Batch   280  of  3,344.    Elapsed: 0:01:29.\n",
      "  Batch   320  of  3,344.    Elapsed: 0:01:42.\n",
      "  Batch   360  of  3,344.    Elapsed: 0:01:55.\n",
      "  Batch   400  of  3,344.    Elapsed: 0:02:08.\n",
      "  Batch   440  of  3,344.    Elapsed: 0:02:20.\n",
      "  Batch   480  of  3,344.    Elapsed: 0:02:33.\n",
      "  Batch   520  of  3,344.    Elapsed: 0:02:46.\n",
      "  Batch   560  of  3,344.    Elapsed: 0:02:59.\n",
      "  Batch   600  of  3,344.    Elapsed: 0:03:11.\n",
      "  Batch   640  of  3,344.    Elapsed: 0:03:24.\n",
      "  Batch   680  of  3,344.    Elapsed: 0:03:37.\n",
      "  Batch   720  of  3,344.    Elapsed: 0:03:50.\n",
      "  Batch   760  of  3,344.    Elapsed: 0:04:02.\n",
      "  Batch   800  of  3,344.    Elapsed: 0:04:15.\n",
      "  Batch   840  of  3,344.    Elapsed: 0:04:28.\n",
      "  Batch   880  of  3,344.    Elapsed: 0:04:41.\n",
      "  Batch   920  of  3,344.    Elapsed: 0:04:53.\n",
      "  Batch   960  of  3,344.    Elapsed: 0:05:06.\n",
      "  Batch 1,000  of  3,344.    Elapsed: 0:05:19.\n",
      "  Batch 1,040  of  3,344.    Elapsed: 0:05:32.\n",
      "  Batch 1,080  of  3,344.    Elapsed: 0:05:44.\n",
      "  Batch 1,120  of  3,344.    Elapsed: 0:05:57.\n",
      "  Batch 1,160  of  3,344.    Elapsed: 0:06:10.\n",
      "  Batch 1,200  of  3,344.    Elapsed: 0:06:23.\n",
      "  Batch 1,240  of  3,344.    Elapsed: 0:06:35.\n",
      "  Batch 1,280  of  3,344.    Elapsed: 0:06:48.\n",
      "  Batch 1,320  of  3,344.    Elapsed: 0:07:01.\n",
      "  Batch 1,360  of  3,344.    Elapsed: 0:07:14.\n",
      "  Batch 1,400  of  3,344.    Elapsed: 0:07:27.\n",
      "  Batch 1,440  of  3,344.    Elapsed: 0:07:39.\n",
      "  Batch 1,480  of  3,344.    Elapsed: 0:07:52.\n",
      "  Batch 1,520  of  3,344.    Elapsed: 0:08:05.\n",
      "  Batch 1,560  of  3,344.    Elapsed: 0:08:17.\n",
      "  Batch 1,600  of  3,344.    Elapsed: 0:08:30.\n",
      "  Batch 1,640  of  3,344.    Elapsed: 0:08:43.\n",
      "  Batch 1,680  of  3,344.    Elapsed: 0:08:56.\n",
      "  Batch 1,720  of  3,344.    Elapsed: 0:09:08.\n",
      "  Batch 1,760  of  3,344.    Elapsed: 0:09:21.\n",
      "  Batch 1,800  of  3,344.    Elapsed: 0:09:34.\n",
      "  Batch 1,840  of  3,344.    Elapsed: 0:09:47.\n",
      "  Batch 1,880  of  3,344.    Elapsed: 0:09:59.\n",
      "  Batch 1,920  of  3,344.    Elapsed: 0:10:12.\n",
      "  Batch 1,960  of  3,344.    Elapsed: 0:10:25.\n",
      "  Batch 2,000  of  3,344.    Elapsed: 0:10:38.\n",
      "  Batch 2,040  of  3,344.    Elapsed: 0:10:50.\n",
      "  Batch 2,080  of  3,344.    Elapsed: 0:11:03.\n",
      "  Batch 2,120  of  3,344.    Elapsed: 0:11:16.\n",
      "  Batch 2,160  of  3,344.    Elapsed: 0:11:29.\n",
      "  Batch 2,200  of  3,344.    Elapsed: 0:11:41.\n",
      "  Batch 2,240  of  3,344.    Elapsed: 0:11:54.\n",
      "  Batch 2,280  of  3,344.    Elapsed: 0:12:07.\n",
      "  Batch 2,320  of  3,344.    Elapsed: 0:12:20.\n",
      "  Batch 2,360  of  3,344.    Elapsed: 0:12:32.\n",
      "  Batch 2,400  of  3,344.    Elapsed: 0:12:45.\n",
      "  Batch 2,440  of  3,344.    Elapsed: 0:12:58.\n",
      "  Batch 2,480  of  3,344.    Elapsed: 0:13:11.\n",
      "  Batch 2,520  of  3,344.    Elapsed: 0:13:24.\n",
      "  Batch 2,560  of  3,344.    Elapsed: 0:13:36.\n",
      "  Batch 2,600  of  3,344.    Elapsed: 0:13:49.\n",
      "  Batch 2,640  of  3,344.    Elapsed: 0:14:02.\n",
      "  Batch 2,680  of  3,344.    Elapsed: 0:14:15.\n",
      "  Batch 2,720  of  3,344.    Elapsed: 0:14:27.\n",
      "  Batch 2,760  of  3,344.    Elapsed: 0:14:40.\n",
      "  Batch 2,800  of  3,344.    Elapsed: 0:14:53.\n",
      "  Batch 2,840  of  3,344.    Elapsed: 0:15:06.\n",
      "  Batch 2,880  of  3,344.    Elapsed: 0:15:18.\n",
      "  Batch 2,920  of  3,344.    Elapsed: 0:15:31.\n",
      "  Batch 2,960  of  3,344.    Elapsed: 0:15:44.\n",
      "  Batch 3,000  of  3,344.    Elapsed: 0:15:57.\n",
      "  Batch 3,040  of  3,344.    Elapsed: 0:16:09.\n",
      "  Batch 3,080  of  3,344.    Elapsed: 0:16:22.\n",
      "  Batch 3,120  of  3,344.    Elapsed: 0:16:35.\n",
      "  Batch 3,160  of  3,344.    Elapsed: 0:16:48.\n",
      "  Batch 3,200  of  3,344.    Elapsed: 0:17:00.\n",
      "  Batch 3,240  of  3,344.    Elapsed: 0:17:13.\n",
      "  Batch 3,280  of  3,344.    Elapsed: 0:17:26.\n",
      "  Batch 3,320  of  3,344.    Elapsed: 0:17:39.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epcoh took: 0:17:46\n",
      "\n",
      "Running Validation...\n",
      "  F1 score: 0.84\n",
      "  Validation took: 0:01:13\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Seed мы задали в самом начале, он не такой, как у авторов\n",
    "# Но у нас и данные другие\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(\n",
    "        epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print(\n",
    "                '  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(\n",
    "                    step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Сlear any previously calculated gradients before performing a\n",
    "        # backward pass.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the\n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(\n",
    "        format_time(time.time() - t0)))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch,\n",
    "    # measure our performance on our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    eval_loss, eval_f1 = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss\n",
    "            # because we have not provided labels.\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the f1 for this batch of test sentences.\n",
    "        tmp_eval_f1 = flat_f1_score(logits, label_ids)\n",
    "\n",
    "        # Accumulate the total f1.\n",
    "        eval_f1 += tmp_eval_f1\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final f1 for this validation run.\n",
    "    print(\"  F1 score: {0:.2f}\".format(eval_f1/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество ровно такое же, как в нашей улучшенной модели :)\n",
    "\n",
    "Визуализируем процесс обучения для полноты картины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T20:25:24.845739Z",
     "iopub.status.busy": "2021-12-20T20:25:24.845141Z",
     "iopub.status.idle": "2021-12-20T20:25:25.130128Z",
     "shell.execute_reply": "2021-12-20T20:25:25.129428Z",
     "shell.execute_reply.started": "2021-12-20T20:25:24.845702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
